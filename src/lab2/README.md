# Лабораторная работа 2: Пересылка данных по кольцу

## Цель работы
Реализовать алгоритм пересылки и суммирования данных между процессорами, расположенными в кольцевой топологии, используя MPJ Express.

## Задание
1. Каждый процессор помещает свой ранг в целочисленную переменную buf.
2. Каждый процессор пересылает переменную buf соседу справа (по часовой стрелке по кольцу).
3. Каждый процессор суммирует принимаемое значение в переменную s, а затем передаёт рассчитанное значение соседу справа.
4. Пересылки по кольцу прекращаются, когда нулевой процессор просуммирует ранги всех процессоров.

## Реализация

### Основная структура программы

```java
import mpi.*;
import java.nio.charset.StandardCharsets;

public class RingSum {
    private static final boolean NON_BLOCKING = false; // Флаг для выбора режима

    public static void main(String[] args) throws Exception {
        MPI.Init(args);

        int rank = MPI.COMM_WORLD.Rank();
        int size = MPI.COMM_WORLD.Size();

        int[] buf = new int;
        int[] s = new int;

        buf = rank;
        s = rank;

        // Код для пересылки данных

        MPI.Finalize();
    }

    // Методы для блокирующего и неблокирующего режимов
}
```

### Блокирующий режим

```java
private static void blockingMode(int rank, int size, int[] buf, int[] s) throws Exception {
    int nextRank = (rank + 1) % size;
    int prevRank = (rank - 1 + size) % size;

    for (int i = 0; i < size - 1; i++) {
        MPI.COMM_WORLD.Sendrecv(buf, 0, 1, MPI.INT, nextRank, 0,
                                buf, 0, 1, MPI.INT, prevRank, 0);

        s += buf;

        if (rank == 0 && s == size * (size - 1) / 2) {
            break;
        }
    }
}
```

### Неблокирующий режим

```java
private static void nonBlockingMode(int rank, int size, int[] buf, int[] s) throws Exception {
    int nextRank = (rank + 1) % size;
    int prevRank = (rank - 1 + size) % size;
    Request sendRequest, recvRequest;
    Status status;

    for (int i = 0; i < size - 1; i++) {
        sendRequest = MPI.COMM_WORLD.Isend(buf, 0, 1, MPI.INT, nextRank, 0);
        recvRequest = MPI.COMM_WORLD.Irecv(buf, 0, 1, MPI.INT, prevRank, 0);

        status = recvRequest.Wait();
        sendRequest.Wait();

        s += buf;

        if (rank == 0 && s == size * (size - 1) / 2) {
            break;
        }
    }
}
```

## Анализ реализации

### Блокирующий режим (Sendrecv)

`MPI.COMM_WORLD.Sendrecv(buf, 0, 1, MPI.INT, nextRank, 0, buf, 0, 1, MPI.INT, prevRank, 0);`

Эта функция выполняет одновременно отправку и получение данных. Параметры:
1. `buf` - буфер для отправки и получения данных
2. `0` - начальный индекс в буфере
3. `1` - количество элементов для отправки/получения
4. `MPI.INT` - тип данных
5. `nextRank` - ранг процесса, которому отправляем
6. `0` - тег сообщения для отправки
7. `buf` - буфер для получения данных (тот же, что и для отправки)
8. `0` - начальный индекс в буфере для получения
9. `1` - количество элементов для получения
10. `MPI.INT` - тип получаемых данных
11. `prevRank` - ранг процесса, от которого получаем
12. `0` - тег сообщения для получения

### Неблокирующий режим (Isend и Irecv)

1. `sendRequest = MPI.COMM_WORLD.Isend(buf, 0, 1, MPI.INT, nextRank, 0);`
    - Инициирует неблокирующую отправку данных следующему процессу.
    - Процесс не ждет завершения отправки и может продолжить выполнение.

2. `recvRequest = MPI.COMM_WORLD.Irecv(buf, 0, 1, MPI.INT, prevRank, 0);`
    - Инициирует неблокирующее получение данных от предыдущего процесса.
    - Процесс не ждет завершения получения и может продолжить выполнение.

3. `status = recvRequest.Wait();` и `sendRequest.Wait();`
    - Ожидают завершения операций получения и отправки соответственно.
    - Гарантируют, что коммуникации завершены перед продолжением.

## Сравнение блокирующего и неблокирующего режимов

1. Инициация операций:
    - Блокирующий: Одна операция `Sendrecv` инициирует и отправку, и получение.
    - Неблокирующий: Отдельные операции `Isend` и `Irecv` инициируют отправку и получение независимо.

2. Время выполнения:
    - Блокирующий: Процесс ждет завершения обеих операций перед продолжением.
    - Неблокирующий: Процесс может выполнять другие операции между инициацией и завершением коммуникаций.

3. Управление завершением:
    - Блокирующий: Автоматически ждет завершения обеих операций.
    - Неблокирующий: Требует явных вызовов `Wait()` для ожидания завершения операций.

4. Потенциал для оптимизации:
    - Блокирующий: Менее гибкий, но проще в использовании.
    - Неблокирующий: Более гибкий, позволяет потенциально перекрывать коммуникации с вычислениями.

5. Сложность кода:
    - Блокирующий: Проще, требует меньше кода.
    - Неблокирующий: Сложнее, требует дополнительного кода для управления запросами.

## Заключение

В данной лабораторной работе были реализованы и сравнены блокирующий и неблокирующий методы пересылки данных по кольцу с использованием MPJ Express. Оба метода успешно решают поставленную задачу, но имеют свои особенности:

- Блокирующий метод проще в реализации и понимании, но может быть менее эффективным в сценариях, где возможно перекрытие коммуникаций и вычислений.
- Неблокирующий метод сложнее в реализации, но предоставляет больше возможностей для оптимизации в сложных сценариях.

Выбор между блокирующим и неблокирующим методами зависит от конкретной задачи и требований к производительности. В простых сценариях, как в данной лабораторной работе, разница в производительности может быть минимальной, но в более сложных приложениях неблокирующие операции могут предоставить значительные преимущества.

## Как запустить программу

1. Скомпилируйте программу:
   ```
   javac -cp .:$MPJ_HOME/lib/mpj.jar RingSum.java
   ```

2. Запустите программу на нескольких процессорах (например, на 6):
   ```
   mpjrun.sh -np 6 RingSum
   ```


